{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, I am going to try to categorize Brown corpus documents into genres using unsupervised learning. To be exact, I am going to perform K-Means clustering. \n",
    "\n",
    "If we do the clustering correctly, we should get groups of documents sorted to genres. Clustering will tell us, which documents, and even which categories, are similar to each other in terms of vectors. We are going to compute purity of the clusters to evaluate how correct the clusters are. We will see if K-Means algorithm is suitable for this type of task and if we can draw any meaningful conclusions based on unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "In unsupervised learning, we do not tell the machine for what patterns to search or how the patterns should look like. Instead, we let the machine look for the patterns independently. \n",
    "\n",
    "K-Means clustering, which I am going to use, is a type of unsupervised learning algorithm. Clustering is grouping similar objects together. In this task, clustering means finding similar documents and dividing them into 15 groups based on the documents' genres (precisely adventure, belles_lettres, editorial, fiction, government, hobbies, humor, learned, lore, mystery, news, religion, reviews, romance, and science fiction). Based on the cluster purity I will decide if the 15 clusters are good enough or we should reduce the number of the clusters.\n",
    "\n",
    "The documents in the cluster should be as similar as possible, whereas the clusters should be as distinct as possible from each other. To find similarities in documents, K-Means algorithm operates with vectors. As Jurafsky and Martin explain: 'a vector is [...] just a list or array of numbers'. We are going to convert each document into a vector, then compare them, and divide them into groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Tools\n",
    "\n",
    "At first, I need to import all resources. I am going to use nltk's Brown corpus as my data. Every document in Brown corpus is already labeled with a category. Unfortunately, the labels are not conveniently named, so I have listed the labels with their meanings below:\n",
    "\n",
    "- ca = news\n",
    "- cb = editorial\n",
    "- cc = reviews\n",
    "- cd = religion\n",
    "- ce = hobbies\n",
    "- cf = lore\n",
    "- cg = belles lettres\n",
    "- ch = government\n",
    "- cj = learned\n",
    "- ck = fiction\n",
    "- cl = mystery\n",
    "- cm = science fiction\n",
    "- cn = adventure\n",
    "- cp = romance\n",
    "- cr = humor\n",
    "\n",
    "During the clustering evaluation, I am going to use `categories` method which displays the category a document belongs to. With this method, it is easier to read the clusters and draw conclusions.\n",
    "\n",
    "From nltk, I am importing the K-Means Clusterer and Euclidean Distance which are both needed for the algorithm itself.\n",
    "\n",
    "For vector creating and handling, I am going to use numpy and random library. To normalize vectors and perform mathematical calculations, I am importing math library.\n",
    "\n",
    "To compute clusters purity, I am importing Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, math, random\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = brown.categories()\n",
    "documents = brown.fileids(categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set partitioning\n",
    "\n",
    "Now, I need to split data into a development set and a final test set. I am going to use the development set fto try different dimensionalities of vectors and different number of clusters to obtain the best results.\n",
    "\n",
    "I am going to split the data in a half. The first half will belong to the development set and the second half to the final test set. The data is not split exactly in the middle. If I did the partitioning this way I would end up having few types of documents in the development set and completely different types of documents in the test set. I am using indexing and putting all odd documents in the development set, whereas all even documents go into the test set.\n",
    "\n",
    "_Side note: Don't get confused by the code below. Indexing starts at 0, but the documents start from '01'. That's why we end up having odd documents in the development set although we iterate over even indexes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "develop_set = [doc for i, doc in enumerate(documents) if i % 2 == 0]\n",
    "test_set = [doc for doc in documents if doc not in develop_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I go through all the documents in the development set and create a vocabulary out of it. We are going to use it when we create vectors. \n",
    "\n",
    "Later, when we create vectors in the final test set, we can encounter a word we have not seen before (e.g. it was not present in the development set). We need to handle this case as well, that is why I am appending `UNK` tag at the end of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for doc in develop_set:\n",
    "    for word in brown.words(doc):\n",
    "        vocab.append(word.lower())\n",
    "vocab = set(vocab)\n",
    "vocab.add('UNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing Vectors and Performing K-Means Clustering\n",
    "\n",
    "Wen we have split our data, we can start developing the vectors and optimize them for K-Means Clustering. We are going to use the `develop_set` for now.\n",
    "\n",
    "To find out which genres (or documents) are similar to each other, we try to aggregate contexts of words. But, storing information about every words takes a lot of storage. That is why we are going to perform random indexing and are going to represent every context word as a vector of a fixed number of dimensions. (These are called _index vectors_) Then we want to sum the index vectors to get a _context vector_ for every document we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Vectors\n",
    "\n",
    "We are going to start creating our vectors by defining the dimensionality of vectors. We set the dimensionality to 1000. For practical reasons, we are going to apply random mapping. When we create our _index vector_ we usually initialize all vector components to zeros. In addition, with random mapping we randomly choose 50 components which we change to ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1000  # Dimensionality\n",
    "m = 50    # Number of non-zero components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vector = { word: [1]*m + [0]*(d-m) for word in vocab }\n",
    "\n",
    "for word in index_vector:\n",
    "    random.shuffle(index_vector[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Vectors\n",
    "\n",
    "Now, we need to create a context vector. The context vector will consist of entire documents. We start by creating a vector consisting of zeros for every document in the `develop_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vector = { doc: [0.0]*d for doc in develop_set }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Vectors\n",
    "\n",
    "We have index and context vectors. But they are random. So far, there is no correlation between related words or documents. To draw any meaningful conclusion, we have to add index vectors together into context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vector(a, b):\n",
    "    '''Add vector b to vector a and store the result in a: a <- a + b'''\n",
    "    for i,x in enumerate(b):\n",
    "        a[i] += x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in develop_set:\n",
    "    for word in brown.words(doc):\n",
    "        add_vector(context_vector[doc], index_vector[word.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "To compare the vectors, we need to normalize them. Now, word frequencies matter and can change the results. We need a function which normalizes a vector by the length (magnitude) of the vector. After normalization, the length of a vector will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a):\n",
    "    total = math.sqrt(sum(x**2 for x in a))\n",
    "    return [x/total for x in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "Finally, everything is prepared for K-Means Clustering. To remind, clustering is grouping similar data points together. K-Means Clusterer divides the data into a desired number of clusters. Each cluster is described by its mean (called 'centroid'). We can compute centroids and divide the data as many times as we want to obtain the cleanest clusters.\n",
    "\n",
    "At first, we want to create a numpy array out of the normalized vectors. We set the number of clusters to 15, as we have 15 genres. Then we initialize the clusterer, so it returns 15 clusters, uses Euclidean distance to compare vectors, avoids creation of empty clusters, and repeats computing centroids and subsequent division of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [ numpy.array(normalize(context_vector[d])) for d in develop_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "clusterer = KMeansClusterer(n_clusters, euclidean_distance, repeats=10, avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(vectors, assign_clusters=True, trace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we got the desired clusters, we cannot tell much from this output. Let's make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 7, 7, 7, 9, 7, 9, 9, 9, 9, 9, 9, 7, 9, 9, 7, 2, 7, 9, 7, 11, 7, 7, 7, 7, 9, 11, 11, 11, 7, 14, 6, 11, 6, 7, 11, 9, 6, 9, 9, 6, 6, 6, 2, 11, 6, 14, 0, 6, 11, 7, 9, 11, 11, 11, 0, 11, 11, 11, 0, 6, 11, 9, 11, 9, 13, 13, 11, 4, 11, 3, 11, 13, 9, 9, 9, 7, 9, 14, 9, 7, 9, 9, 9, 6, 9, 4, 9, 6, 2, 6, 2, 7, 2, 2, 3, 9, 6, 7, 14, 6, 2, 6, 11, 11, 7, 0, 6, 11, 9, 9, 8, 6, 3, 2, 2, 2, 7, 9, 9, 9, 6, 6, 3, 7, 9, 9, 6, 14, 2, 9, 4, 2, 2, 5, 2, 2, 11, 11, 7, 7, 2, 11, 6, 11, 2, 0, 0, 11, 0, 9, 11, 2, 1, 2, 11, 2, 1, 2, 14, 2, 9, 11, 4, 7, 6, 0, 10, 0, 11, 12, 2, 6, 9, 2, 6, 6, 6, 6, 2, 0, 11, 4, 0, 0, 0, 8, 3, 3, 8, 9, 3, 3, 3, 8, 3, 3, 9, 9, 7, 9, 8, 3, 3, 8, 8, 8, 8, 3, 3, 8, 9, 8, 9, 8, 3, 3, 3, 3, 9, 8, 8, 3, 8, 8, 3, 3, 8, 3, 3, 3, 8, 9, 8, 3, 8, 9, 8, 9, 8, 9, 8, 3, 8, 8, 9, 3, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: religion, hobbies, hobbies, belles_lettres, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #1: learned, learned\n",
      "Cluster #2: news, religion, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, government, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #3: lore, belles_lettres, belles_lettres, belles_lettres, fiction, fiction, fiction, fiction, fiction, fiction, fiction, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, adventure, adventure, adventure, adventure, adventure, romance, romance, romance, humor\n",
      "Cluster #4: hobbies, lore, government, learned, learned\n",
      "Cluster #5: government\n",
      "Cluster #6: editorial, editorial, reviews, reviews, reviews, reviews, religion, religion, hobbies, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, learned, learned, learned, learned, learned, learned, humor\n",
      "Cluster #7: news, news, news, news, news, news, news, news, news, news, editorial, editorial, editorial, editorial, editorial, editorial, religion, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, learned, fiction\n",
      "Cluster #8: belles_lettres, fiction, fiction, fiction, mystery, mystery, mystery, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, adventure, adventure, romance, romance, romance, romance, romance, romance, romance, romance, humor\n",
      "Cluster #9: news, news, news, news, news, news, news, news, news, news, editorial, reviews, reviews, reviews, religion, hobbies, hobbies, lore, lore, lore, lore, lore, lore, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, learned, learned, fiction, fiction, fiction, fiction, mystery, science_fiction, adventure, romance, romance, romance, romance, humor\n",
      "Cluster #10: learned\n",
      "Cluster #11: news, editorial, editorial, editorial, editorial, reviews, religion, religion, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, lore, belles_lettres, belles_lettres, belles_lettres, government, government, government, government, learned, learned, learned, learned, learned, learned\n",
      "Cluster #12: learned\n",
      "Cluster #13: hobbies, hobbies, lore\n",
      "Cluster #14: editorial, religion, lore, belles_lettres, belles_lettres, learned\n"
     ]
    }
   ],
   "source": [
    "clusters_again = { key: [] for key in range(n_clusters) }\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_again[clusters[i]].append(brown.categories(develop_set[i])[0])\n",
    "\n",
    "for key in clusters_again:\n",
    "    print(\"Cluster #\", key, \": \", \", \".join(val for val in clusters_again[key]) , sep=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the desired 15 clusters. However, they do not look promising. There are some clusters having just one document, whereas others have many. The division of the documents also does not look right. \n",
    "\n",
    "To evaluate the result easier, we can compute cluster purity. Based on the result of cluster purity, we can decide if we need to change anything or the clusters are sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Purity\n",
    "\n",
    "From Wikipedia: Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster. Now take the sum over all clusters and divide by the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity(clusters):\n",
    "    most_common_class_sum = 0\n",
    "    total_data_points = 0\n",
    "    for clust, docs in clusters.items():\n",
    "        c = Counter(docs)\n",
    "        for i, d in enumerate(c.items()):\n",
    "            if i == 0:\n",
    "                most_common_class_sum += d[1]\n",
    "            total_data_points += d[1]\n",
    "    return most_common_class_sum / total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.144"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity(clusters_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got cluster purity around 15% which is actually pretty bad. \n",
    "\n",
    "One thing we could do to get better results is to increase the number of vector dimensions. However, it would take a lot of time and probably would not help us that much. So, I am not going to change the number of dimensions or the number of non-zero components.\n",
    "\n",
    "Another thing we can try is to change the number of repeats during the clustering process. We can set both lower and higher numbers to see if it will increase the cluster purity. I think repeating the clustering process less than 10 times will leave the purity as it is or even decrease it but I still want to test it. I am not sure what higher number will do and I do not dare to guess so we will test it, too.\n",
    "\n",
    "Last, we can play with the number of clusters. From what we have read about the purity, we can assume that decreasing the number of clusters might also decrease the purity. On the other hand, increasing the number of cluster might increase the cluster purity. But there is a problem with the second case which lies in the calculation itself. We can obtain purity of 1 easily by putting each document in its own class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Number of Repeats\n",
    "\n",
    "Let's start by changing the `repeats` parameter to 5. Except for the clusterer everything stays the same so the only thing we need to do is to initialize the clusterer once again, perform the clustering, print the obtained clusters and check the purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: news, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #1: fiction, fiction, mystery, science_fiction, adventure, adventure, adventure, romance\n",
      "Cluster #2: belles_lettres, fiction, fiction, fiction, fiction, fiction, fiction, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, adventure, adventure, adventure, romance, romance, humor\n",
      "Cluster #3: news, news, news, news, lore, lore, lore, learned, fiction, fiction, fiction, fiction, romance, romance\n",
      "Cluster #4: news, news, news, news, news, news, editorial, reviews, reviews, reviews, reviews, reviews, reviews, reviews, religion, hobbies, hobbies, hobbies, hobbies, lore, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, humor\n",
      "Cluster #5: government\n",
      "Cluster #6: news, news, news, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, religion, religion, hobbies, hobbies, hobbies, hobbies, lore, belles_lettres, government, government, government, learned, learned\n",
      "Cluster #7: news, editorial, reviews, religion, religion, religion, hobbies, hobbies, hobbies, belles_lettres, belles_lettres, belles_lettres, government, government, government, government, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #8: mystery, mystery, mystery, adventure, romance, romance, romance, romance, romance, romance\n",
      "Cluster #9: news, religion, belles_lettres, fiction, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, romance, romance, romance, humor, humor\n",
      "Cluster #10: news, news, news, news\n",
      "Cluster #11: news, editorial, hobbies, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, fiction, fiction, romance\n",
      "Cluster #12: news, editorial, editorial, hobbies, hobbies, hobbies, hobbies, lore, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, learned, adventure\n",
      "Cluster #13: religion, religion, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned\n",
      "Cluster #14: hobbies, hobbies, lore\n",
      "\n",
      "0.132\n"
     ]
    }
   ],
   "source": [
    "clusterer = KMeansClusterer(n_clusters, euclidean_distance, repeats=5, avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(vectors, assign_clusters=True, trace=False)\n",
    "\n",
    "clusters_again = { key: [] for key in range(n_clusters) }\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_again[clusters[i]].append(brown.categories(develop_set[i])[0])\n",
    "\n",
    "for key in clusters_again:\n",
    "    print(\"Cluster #\", key, \": \", \", \".join(val for val in clusters_again[key]) , sep=\"\")\n",
    "    \n",
    "print()\n",
    "print(purity(clusters_again))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I expected, the purity decreased. But what will increasing `repeat` parameter to e.g. 20 do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: learned, learned, learned, learned, learned, learned\n",
      "Cluster #1: hobbies, hobbies, government\n",
      "Cluster #2: lore, belles_lettres, government, government, government, learned\n",
      "Cluster #3: news, religion, religion, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #4: news, news, editorial, reviews, reviews, hobbies, hobbies, hobbies, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, learned, learned, learned, learned, learned, learned, humor\n",
      "Cluster #5: belles_lettres, belles_lettres, belles_lettres, belles_lettres, fiction, fiction, fiction, fiction, fiction, fiction, fiction, fiction, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, adventure, adventure, adventure, adventure, romance, romance, humor\n",
      "Cluster #6: fiction, fiction, mystery, mystery, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, romance, romance, romance, romance, romance, romance, romance\n",
      "Cluster #7: news, news, news, news, news, news, news, news, news, editorial, religion, religion, lore, lore, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, learned, fiction, fiction, romance\n",
      "Cluster #8: learned\n",
      "Cluster #9: government, learned, learned\n",
      "Cluster #10: news, editorial, editorial, editorial, reviews, reviews, reviews, hobbies, hobbies, hobbies, hobbies, lore, lore, lore, lore, lore, belles_lettres, learned, mystery, science_fiction, adventure, romance, romance, humor\n",
      "Cluster #11: news, news, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, reviews, reviews, reviews, religion, religion, religion, hobbies, hobbies, hobbies, hobbies, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, government, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #12: hobbies, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, fiction, fiction, romance, romance, humor\n",
      "Cluster #13: news, news, news, news, news, news, news, editorial, hobbies, hobbies, hobbies, lore\n",
      "Cluster #14: religion, religion, hobbies, lore, lore, belles_lettres, belles_lettres, fiction, mystery, adventure, adventure, adventure, romance\n",
      "\n",
      "0.168\n"
     ]
    }
   ],
   "source": [
    "clusterer = KMeansClusterer(n_clusters, euclidean_distance, repeats=20, avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(vectors, assign_clusters=True, trace=False)\n",
    "\n",
    "clusters_again = { key: [] for key in range(n_clusters) }\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_again[clusters[i]].append(brown.categories(develop_set[i])[0])\n",
    "\n",
    "for key in clusters_again:\n",
    "    print(\"Cluster #\", key, \": \", \", \".join(val for val in clusters_again[key]) , sep=\"\")\n",
    "    \n",
    "print()\n",
    "print(purity(clusters_again))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purity is higher but not much. We still get almost the same result. I tried higher numbers (e.g. 50), too. But they make no difference on the purity either. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Number of Clusters\n",
    "\n",
    "When changing the number of `repeats` did not work, we have to try getting different number of clusters than we originally intended. We will keep `repeats` set to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: news, religion, hobbies, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #1: news, news, news, news, editorial, religion, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, fiction, fiction\n",
      "Cluster #2: belles_lettres, belles_lettres, belles_lettres, belles_lettres, fiction, fiction, fiction, fiction, fiction, fiction, fiction, fiction, fiction, fiction, mystery, mystery, mystery, mystery, mystery, mystery, mystery, science_fiction, science_fiction, adventure, adventure, adventure, adventure, adventure, adventure, adventure, adventure, adventure, adventure, adventure, romance, romance, romance, romance, humor\n",
      "Cluster #3: editorial, reviews, reviews, reviews, reviews, religion, religion, religion, hobbies, hobbies, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, government, government, government, government, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #4: news, news, news, news, news, news, news, news, news, news, editorial, reviews, reviews, reviews, reviews, religion, hobbies, hobbies, hobbies, lore, lore, lore, lore, lore, lore, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, learned, learned, learned, learned, learned, fiction, fiction, romance, humor\n",
      "Cluster #5: news, news, news, news, news, news, news, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, editorial, religion, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, hobbies, lore, lore, lore, belles_lettres, belles_lettres, government, government, government, learned, learned, learned, learned\n",
      "Cluster #6: religion, religion, hobbies, hobbies, lore, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, fiction, mystery, mystery, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, romance, romance, romance, romance, romance, romance, romance, romance, romance, romance, humor, humor\n",
      "\n",
      "0.116\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 7\n",
    "clusterer = KMeansClusterer(n_clusters, euclidean_distance, repeats=10, avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(vectors, assign_clusters=True, trace=False)\n",
    "\n",
    "clusters_again = { key: [] for key in range(n_clusters) }\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_again[clusters[i]].append(brown.categories(develop_set[i])[0])\n",
    "\n",
    "for key in clusters_again:\n",
    "    print(\"Cluster #\", key, \": \", \", \".join(val for val in clusters_again[key]) , sep=\"\")\n",
    "    \n",
    "print()\n",
    "print(purity(clusters_again))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having less clusters did not improve the cluster purity at all. It is now even harder to get a single class in one cluster and the classes are spread among all clusters, so the purity is even lower than before.\n",
    "\n",
    "But we can still test our hypothesis of increasing the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: hobbies, hobbies, government\n",
      "Cluster #1: lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #2: lore, learned\n",
      "Cluster #3: learned\n",
      "Cluster #4: editorial, reviews, religion, hobbies, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, government, learned, learned, learned, learned, learned, learned, learned, humor\n",
      "Cluster #5: belles_lettres, belles_lettres, fiction, fiction, fiction, fiction, fiction, mystery, mystery, science_fiction, adventure, adventure, adventure, adventure, adventure, adventure, romance\n",
      "Cluster #6: fiction, fiction, fiction, fiction, mystery, mystery, mystery, science_fiction, adventure, adventure, adventure, adventure, romance, romance\n",
      "Cluster #7: hobbies, government\n",
      "Cluster #8: government\n",
      "Cluster #9: lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, fiction, fiction, mystery, romance\n",
      "Cluster #10: mystery, mystery, mystery, mystery, romance, romance, romance, romance, romance\n",
      "Cluster #11: news, religion, religion, lore, belles_lettres, belles_lettres, government, government, learned, learned\n",
      "Cluster #12: government\n",
      "Cluster #13: news, news, news, news, news, news, news, editorial, editorial, editorial, editorial, editorial, religion, lore, lore, lore, belles_lettres, belles_lettres, belles_lettres, government, fiction\n",
      "Cluster #14: editorial, hobbies, hobbies, belles_lettres, belles_lettres, learned\n",
      "Cluster #15: mystery, adventure, adventure\n",
      "Cluster #16: news, news, news, news, news, hobbies, belles_lettres, learned, learned\n",
      "Cluster #17: learned\n",
      "Cluster #18: religion, lore, lore, lore, lore, belles_lettres, belles_lettres, learned, fiction, fiction, mystery, science_fiction, adventure, adventure, romance, romance, romance, humor\n",
      "Cluster #19: news, news, news, news, news, editorial, reviews, reviews, reviews, hobbies, hobbies, hobbies, hobbies, hobbies, lore, lore, lore, lore, belles_lettres, government, humor\n",
      "Cluster #20: belles_lettres, learned\n",
      "Cluster #21: editorial, reviews, reviews, reviews, reviews, religion, religion, hobbies, hobbies, hobbies, lore, lore, lore, belles_lettres, belles_lettres, learned, learned, learned, learned, learned\n",
      "Cluster #22: belles_lettres, fiction, romance, romance, romance, humor\n",
      "Cluster #23: news, news, news, editorial, editorial, editorial, editorial, editorial, religion, religion, hobbies, lore, belles_lettres, belles_lettres, government, government, learned, learned\n",
      "Cluster #24: news, hobbies, hobbies, belles_lettres, government\n",
      "\n",
      "0.212\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 25\n",
    "clusterer = KMeansClusterer(n_clusters, euclidean_distance, repeats=10, avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(vectors, assign_clusters=True, trace=False)\n",
    "\n",
    "clusters_again = { key: [] for key in range(n_clusters) }\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_again[clusters[i]].append(brown.categories(develop_set[i])[0])\n",
    "\n",
    "for key in clusters_again:\n",
    "    print(\"Cluster #\", key, \": \", \", \".join(val for val in clusters_again[key]) , sep=\"\")\n",
    "    \n",
    "print()\n",
    "print(purity(clusters_again))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we got a slighty better result. However, accuracy around 20 % is still not very satisfying. On the other hand, I do not want to have many clusters with just one document to obtain better results. I confirmed that putting a single document into one cluster helps a lot (I tried it with 100 clusters and got 50% accuracy). \n",
    "\n",
    "We will keep 25 clusters for our final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "After developing our algorithm, we can test it on our `test_set`. \n",
    "\n",
    "We will do a very small change to our code. Our vocabulary is based on the training data and if a word that did not appear in the training data occurs we need to handle it somehow. We created the `UNK` index vector for this case. So if the unknown word appears it gets added to the `UNK` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1000\n",
    "m = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_vector = { word: [1]*m + [0]*(d-m) for word in vocab }\n",
    "\n",
    "for word in index_vector:\n",
    "    random.shuffle(index_vector[word])\n",
    "    \n",
    "context_vector = { doc: [0.0]*d for doc in test_set }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in test_set:\n",
    "    for word in brown.words(doc):\n",
    "        if word in index_vector:\n",
    "            add_vector(context_vector[doc], index_vector[word.lower()])\n",
    "        else:\n",
    "            add_vector(context_vector[doc], index_vector['UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [ numpy.array(normalize(context_vector[d])) for d in test_set ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 25\n",
    "clusterer = KMeansClusterer(n_clusters, euclidean_distance, repeats=10, avoid_empty_clusters=True)\n",
    "clusters = clusterer.cluster(vectors, assign_clusters=True, trace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: news, news, reviews, hobbies, government, learned, learned\n",
      "Cluster #1: news, news, editorial, hobbies, lore, lore, belles_lettres, government, learned, learned, learned, learned\n",
      "Cluster #2: news, news, news, news, news, news, news, news, news, news, editorial, editorial, editorial, reviews, reviews, reviews, hobbies, lore, lore, belles_lettres, belles_lettres, government, government, government\n",
      "Cluster #3: government\n",
      "Cluster #4: news, news, news, editorial, editorial, editorial, editorial, editorial, reviews, reviews, reviews, religion, religion, religion, hobbies, lore, belles_lettres, belles_lettres, government, government, learned, learned, learned, learned, learned, learned, learned\n",
      "Cluster #5: news, lore, belles_lettres, belles_lettres, belles_lettres, fiction, fiction, fiction, fiction, mystery, mystery, mystery, mystery, adventure, adventure, adventure, adventure, adventure\n",
      "Cluster #6: belles_lettres, fiction, mystery, adventure, adventure, romance, romance, romance, romance, romance, romance, romance, humor\n",
      "Cluster #7: news, news, editorial, editorial, editorial, reviews, religion, hobbies, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, government, learned, learned, learned, learned, learned\n",
      "Cluster #8: fiction, fiction\n",
      "Cluster #9: government, learned, learned\n",
      "Cluster #10: hobbies, hobbies, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned, fiction, adventure, romance, romance, humor, humor\n",
      "Cluster #11: hobbies, hobbies, hobbies, hobbies\n",
      "Cluster #12: news, religion, hobbies, lore, lore, lore, belles_lettres, belles_lettres, learned, learned, learned\n",
      "Cluster #13: fiction, fiction, fiction, science_fiction, science_fiction, adventure, adventure, adventure, romance, romance, romance\n",
      "Cluster #14: lore, mystery, mystery, mystery, mystery, adventure, romance, humor\n",
      "Cluster #15: belles_lettres, belles_lettres, learned\n",
      "Cluster #16: romance\n",
      "Cluster #17: religion, lore, lore, lore, belles_lettres, belles_lettres, government, government, learned, learned\n",
      "Cluster #18: editorial, editorial, fiction, fiction, fiction, fiction, mystery, mystery, mystery, science_fiction, adventure, adventure, romance\n",
      "Cluster #19: religion, lore, lore, belles_lettres, learned\n",
      "Cluster #20: hobbies, belles_lettres\n",
      "Cluster #21: belles_lettres, learned, learned, learned, learned\n",
      "Cluster #22: news, religion, lore, lore, belles_lettres, belles_lettres, belles_lettres, belles_lettres, learned\n",
      "Cluster #23: religion, belles_lettres, learned, learned\n",
      "Cluster #24: hobbies, hobbies, hobbies, hobbies, hobbies, lore, lore, lore, lore, belles_lettres, belles_lettres, government, government, government, learned, learned, learned, learned, learned\n",
      "\n",
      "0.208\n"
     ]
    }
   ],
   "source": [
    "clusters_again = { key: [] for key in range(n_clusters) }\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    clusters_again[clusters[i]].append(brown.categories(develop_set[i])[0])\n",
    "\n",
    "for key in clusters_again:\n",
    "    print(\"Cluster #\", key, \": \", \", \".join(val for val in clusters_again[key]) , sep=\"\")\n",
    "    \n",
    "print()\n",
    "print(purity(clusters_again))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cluster purity of the test set is almost the same as the cluster purity of the \"final\" development set. The purity is very low, around 20% depending on the current run of the algorithm.\n",
    "\n",
    "From what we have seen, I would say K-Means Clusterer is not a good algorithm for this particular dataset. The clusters are not clean enough, there are document categories spread among all clusters. The cleanest clusters consist of one or two documents (which should not happen).\n",
    "\n",
    "There might be several problems why this dataset is not suitable for this algorithm. First, the document categories do not contain the same number of documents. It is hard to categorize the documents correctly when some categories contain almost 8 times more documents than other categories. Second, the length of the documents might not be sufficient to properly distinguish the categories. If there is not a huge number of unique words for each category, it is harder to cluster the documents correctly. Third, we think 15 is optimal number of clusters because we know the sorted categories of Brown corpus. However, this do not mean 15 is the optimal number of clusters for the clusterer.\n",
    "\n",
    "Despite this poor performance, I cannot say K-Means is a bad algorithm. I think if the dataset was bigger or optimized so it had same number of documents for each category, the clusterer could do a better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
