{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Random-Indexing\">Random Indexing<a class=\"anchor-link\" href=\"#Random-Indexing\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By **Robert Östling**\n",
    "\n",
    "<p>After the third presenter in a row quoted Firth's famous words, \"<em>You shall know a word by the company it keeps</em>\", a colleague sitting next to me at a conference whispered \"<em>not again!</em>\"</p>\n",
    "<p>Even though it may be overused, this observation underlies the field of distributional semantics, where we try to aggregate the contexts of words in order to approximate their meaning.</p>\n",
    "<p>We start with an example. Let us make a list of the words occurring within two words of the word <em>four</em> in the Brown corpus. The first step is to extract all <em>context windows</em>, in this case 5-grams:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'Fulton', 'County', 'Grand', 'Jury'),\n",
       " ('Fulton', 'County', 'Grand', 'Jury', 'said'),\n",
       " ('County', 'Grand', 'Jury', 'said', 'Friday'),\n",
       " ('Grand', 'Jury', 'said', 'Friday', 'an'),\n",
       " ('Jury', 'said', 'Friday', 'an', 'investigation'),\n",
       " ('said', 'Friday', 'an', 'investigation', 'of'),\n",
       " ('Friday', 'an', 'investigation', 'of', \"Atlanta's\"),\n",
       " ('an', 'investigation', 'of', \"Atlanta's\", 'recent')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "n = 5\n",
    "\n",
    "text = list(nltk.corpus.brown.words())\n",
    "vocabulary = set(text)\n",
    "\n",
    "brown_ngrams = list(ngrams(text, n))\n",
    "\n",
    "brown_ngrams[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Next, we gather for each word a list of other words which occur in the same 5-gram.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# neighbors['focus']['context'] will contain the number of times the word 'context' occurs in the same n-gram as 'focus'\n",
    "neighbors = defaultdict(Counter)\n",
    "\n",
    "# Compute the position of the middle word in an n-gram (this is the focus word)\n",
    "middle_position = n // 2\n",
    "\n",
    "# For each n-gram, add the cooccurrence statistics:\n",
    "for ngram in brown_ngrams:\n",
    "    # This is the focus word\n",
    "    focus = ngram[middle_position]\n",
    "    # These are all the words _except_ the focus word\n",
    "    context = ngram[:middle_position] + ngram[middle_position+1:]\n",
    "    for word in context:\n",
    "        neighbors[focus][word] += 1\n",
    "\n",
    "# Now we can answer the original question:\n",
    "print(', '.join('\"%s\"' % word for word,_ in neighbors['four'].most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Now, let's look at a different word, <em>five</em>:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join('\"%s\"' % word for word,_ in neighbors['five'].most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>And a third word, <em>dog</em>:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(', '.join('\"%s\"' % word for word,_ in neighbors['dog'].most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Note that the lists for <em>four</em> and <em>five</em> are more similar to each other than to that of <em>dog</em>.</p>\n",
    "<p>I should note that looking at the top neighbors is a very crude measure of similarity of contexts, below we will discuss better ways of doing this.</p>\n",
    "<p>So far we used only an intuitive sense of similarity, the next step is to formalize this. To begin with, we simply choose the number of overlapping words among the 100 most common neighbors:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the number of overlapping words in the top 100 list of contexts\n",
    "def top100_similarity(word1, word2):\n",
    "    return len({word for word,_ in neighbors[word1].most_common(100)} &\n",
    "               {word for word,_ in neighbors[word2].most_common(100)})\n",
    "\n",
    "# Print the similarity between each pair of words in the given list\n",
    "def pairwise_similarity(words):\n",
    "    for word1 in words:\n",
    "        for word2 in words:\n",
    "            print('%-8s %-8s %4d' % (word1, word2, top100_similarity(word1, word2)))\n",
    "            \n",
    "pairwise_similarity('three four dog'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>As we can see, each word is (obviously) most similar to itself, but <em>three</em> and <em>four</em> indeed have more overlapping contexts than <em>three</em> and <em>dog</em> or <em>four</em> and <em>dog</em>.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Adding-randomness\">Adding randomness<a class=\"anchor-link\" href=\"#Adding-randomness\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>In distributional semantics we normally work with large corpora, with billions or even trillions of words. This means we want to minimize the amount of storage and computation needed for each word. The key theorem behind Random Indexing says that you can represent every context word as a random vector of a fixed number of dimensions (usually around 1000), and then represent a word by the sum of these vectors (instead of the top 100 lists used above).</p>\n",
    "<p>First, let's create a random vector for each word in our vocabulary, these are termed <em>index vectors</em>:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Dimensionality of vectors. Normally this is a large number, but for demonstration purposes we use a smaller number.\n",
    "d = 5\n",
    "\n",
    "# We represent a vector using a list of float values.\n",
    "def random_vector():\n",
    "    return [random.random() for _ in range(d)]\n",
    "\n",
    "index_vector = { word: random_vector() for word in vocabulary }\n",
    "\n",
    "[index_vector[word] for word in 'four five dog'.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>These vectors are random, and there is no correlation between related words. This correlation will appear when we add the index vectors together into <em>context vectors</em>.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with zero vectors\n",
    "context_vector = { word: [0.0]*d for word in vocabulary }\n",
    "\n",
    "# Add vector b to vector a and store the result in a:\n",
    "# a <- a + b\n",
    "def add_vector(a, b):\n",
    "    for i,x in enumerate(b):\n",
    "        a[i] += x\n",
    "\n",
    "# This is almost exactly the same as we did earlier,\n",
    "# except now we modify the context_vector object instead of the neighbors object.\n",
    "for ngram in brown_ngrams:\n",
    "    # This is the focus word\n",
    "    focus = ngram[middle_position]\n",
    "    # These are all the words _except_ the focus word\n",
    "    context = ngram[:middle_position] + ngram[middle_position+1:]\n",
    "    for word in context:\n",
    "        add_vector(context_vector[focus], index_vector[word])\n",
    "\n",
    "[context_vector[word] for word in 'four five dog'.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The values of all of these vectors are quite different, but that's actually just because the words have different frequencies. Let us look what happens if we normalize the vectors, so that the sum of each is 1:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize a vector so that the sum of its elements is 1\n",
    "# We also round the result to 3 decimals, so it looks prettier when we print it,\n",
    "# but this is not necessary.\n",
    "def normalize(a):\n",
    "    total = sum(a)\n",
    "    return [round(x/total, 3) for x in a]\n",
    "\n",
    "[normalize(context_vector[word]) for word in 'four five dog'.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Now the first two (representing <em>four</em> and <em>five</em>) are quite similar! The <em>dog</em> vector is not that far off either, but again we can quantify the distance measure to avoid subjective bias:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Compute the Euclidean distance between vectors a and b\n",
    "def euclidean(a, b):\n",
    "    return math.sqrt(sum((x-y)*(x-y) for x,y in zip(a,b)))\n",
    "\n",
    "# Print the similarity between each pair of words in the given list\n",
    "def pairwise_distance(words):\n",
    "    for word1 in words:\n",
    "        for word2 in words:\n",
    "            print('%-8s %-8s %.3f' % (word1, word2, euclidean(\n",
    "                normalize(context_vector[word1]),\n",
    "                normalize(context_vector[word2]))))\n",
    "            \n",
    "pairwise_distance('three four dog'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>For demonstration purposes we have used only pure Python, but in practice is it easier and faster to use the <em>numpy</em> and <em>scipy</em> libraries.</p>\n",
    "<p>In particular, the module <a href=\"http://docs.scipy.org/doc/scipy/reference/spatial.distance.html\">scipy.spatial.distance</a> contains functions for a wide variety of different similarity measures (including Euclidean distance, as shown above, and cosine similarity, which is perhaps more commonly used in practice).</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
