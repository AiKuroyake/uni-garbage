{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise set 4\n",
    "\n",
    "In this exercise set we will learn more about measuring words and sentences in their context.\n",
    "\n",
    "## Ex 1: Simple co-occurrence similarity measure \n",
    "\n",
    "We practiced calculating n-grams in the exercise set 2. In this exercise, we will learn how to utilize them. \n",
    "\n",
    "a) Following code extracts 2-grams from Anna Karenina -book. Create a dictionary which contains all co-occurring word's counts with the word \"Anna\". \"Anna\" can be either the first or second member of the bigram. Finally, sort and print the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Project'], ['Project', 'Gutenberg'], ['Gutenberg', 'EBook'], ['EBook', 'of'], ['of', 'Anna'], ['Anna', 'Karenina'], ['Karenina', 'by'], ['by', 'Leo'], ['Leo', 'Tolstoy'], ['Tolstoy', 'This'], ['This', 'eBook'], ['eBook', 'is'], ['is', 'for'], ['for', 'the'], ['the', 'use'], ['use', 'of'], ['of', 'anyone'], ['anyone', 'anywhere'], ['anywhere', 'at'], ['at', 'no'], ['no', 'cost'], ['cost', 'and'], ['and', 'with'], ['with', 'almost'], ['almost', 'no'], ['no', 'restrictions'], ['restrictions', 'whatsoever'], ['whatsoever', 'END'], ['END', 'You'], ['You', 'may'], ['may', 'copy'], ['copy', 'it'], ['it', 'give'], ['give', 'it'], ['it', 'away'], ['away', 'or'], ['or', 're'], ['re', 'use'], ['use', 'it'], ['it', 'under'], ['under', 'the'], ['the', 'terms'], ['terms', 'of'], ['of', 'the'], ['the', 'Project'], ['Project', 'Gutenberg'], ['Gutenberg', 'License'], ['License', 'included'], ['included', 'with'], ['with', 'this'], ['this', 'eBook'], ['eBook', 'or'], ['or', 'online'], ['online', 'at'], ['at', 'www'], ['www', 'END'], ['END', 'gutenberg'], ['gutenberg', 'END'], ['END', 'org'], ['org', 'Title'], ['Title', 'Anna'], ['Anna', 'Karenina'], ['Karenina', 'Author'], ['Author', 'Leo'], ['Leo', 'Tolstoy'], ['Tolstoy', 'Release'], ['Release', 'Date'], ['Date', 'July'], ['July', '01'], ['01', '1998'], ['1998', 'EBook'], ['EBook', '1399'], ['1399', 'Last'], ['Last', 'Updated'], ['Updated', 'May'], ['May', '22'], ['22', '2020'], ['2020', 'Language'], ['Language', 'English'], ['English', 'Character'], ['Character', 'set'], ['set', 'encoding'], ['encoding', 'UTF'], ['UTF', '8'], ['8', 'START'], ['START', 'OF'], ['OF', 'THIS'], ['THIS', 'PROJECT'], ['PROJECT', 'GUTENBERG'], ['GUTENBERG', 'EBOOK'], ['EBOOK', 'ANNA'], ['ANNA', 'KARENINA'], ['KARENINA', 'Produced'], ['Produced', 'by'], ['by', 'David'], ['David', 'Brannan'], ['Brannan', 'Andrew'], ['Andrew', 'Sly'], ['Sly', 'and'], ['and', 'David']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "import re\n",
    " \n",
    "f = open(\"anna_karenina.txt\", \"r\").read()\n",
    "\n",
    "f = f.replace(\".\", \" END \")\n",
    "f = f.replace(\"!\", \" END \")\n",
    "f = f.replace(\"?\", \" END \")\n",
    "words = re.split(r'\\W+', f)\n",
    "words.remove('')\n",
    "\n",
    "def word_tokenize(data):\n",
    "    doc = nlp(data)\n",
    "    return list(map(lambda x: x.text, doc))\n",
    " \n",
    "\n",
    "def extract_ngrams(words, num):\n",
    "    ngram_list = []\n",
    "    # Write code in here, the result should contain ngrams as string like \"This is\", \"is an\" etc.\n",
    "    for i in range(0, len(words)-1):\n",
    "        ngram_list.append([words[i], words[i+1]])\n",
    "    return ngram_list\n",
    " \n",
    "ngram = extract_ngrams(words, 2)\n",
    "\n",
    "print(ngram[0:100])\n",
    "\n",
    "def create_dictionary(keyword):\n",
    "    #write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Create same kind of dictionary for the word \"Vronsky\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5c1cd70ff552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vronsky'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "counts = create_dictionary('Vronsky')\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Create same kind of dictionary for \"Moscow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e69db114439b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Moscow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "counts = create_dictionary('Moscow')\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, that \n",
    "\n",
    "1) there are more words in 'Vronsky' and 'Anna'. One reason for that is that there are more occurrences of them in the original text\n",
    "\n",
    "2) The words in each list differ from each others. In 'Moscow' list, words 'to', 'from' and 'leave' are included in high position of the list. That's because Moscow is a city. In both 'Vronsky' and 'Anna' we can see many verbs: Anna said, had and answered often, and Vronsky too. \n",
    "\n",
    "Hopefully these observations demonstrated what's the idea of comparing contexts with co-occurring words. The methods used nowadays (word2vec, GloVe etc.) are more complex methods but have the same base idea, comparing word contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2: Stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the results of last exercise there are lots of non-informative words, such as 's' and 'of'. A common technique to get rid of them is to use *stopword lists*. Following code loads nltk's English stopword list and prints it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Modify your exercise 1 code to first remove every stopword occurrence from the text. If you haven't done exercise 1, you can just code the stopword removal into exercise 1 code. What differences you see now in the results? Are there some words which should be included in the stopword list? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
