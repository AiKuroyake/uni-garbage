{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Lemmatization and POS tagging\n",
    "\n",
    "In this class, we will start working with *language processing pipelines* which enrich our text with useful linguistic annotations. The library that we're going to use is **spaCy** (https://spacy.io/).\n",
    "\n",
    "Let's start by reading the text of \"Anna Karenina\". Instead of storing it as a single huge string, we will read it as a list of paragraphs, each paragraph being a string. The paragaraphs are separated in the file by blank lines which appear as `\\n\\n` in the string representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_paragraphs(filename):\n",
    "    '''Reads a text divided into paragraphs.'''\n",
    "    pars = []\n",
    "    with open(filename) as fp:\n",
    "        text = fp.read()\n",
    "        pars = [p.replace('\\n', ' ') for p in text.split('\\n\\n') if p.strip()]\n",
    "    return pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_paragraphs('anna_karenina.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load a spaCy model. The model is typically stored in a variable called `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process a chunk of text, we simply call `nlp` as a function on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(par) for par in text]  # Run the NLP pipeline paragraph by paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see 11925638236994514241\n"
     ]
    }
   ],
   "source": [
    "print(docs[100][0].lemma_, docs[100][0].lemma)\n",
    "# Prints the first word of a 100th paragraph\n",
    "# .lemma_ prints the word itself, .lemma prints its number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `docs` is a list of `Doc` (\"document\") objects, each document containing a list of tokens. A token contains many useful annotations in addition to its string form. Today we are going to look into the following annotations:\n",
    "* `orth_` - the original string form of the word,\n",
    "* `norm_` - the normalized string form (e.g. lowercased)\n",
    "* `lemma_` - the \"dictionary form\", e.g. `\"did\"` is lemmatized to `\"do\"`\n",
    "* `pos_` - part-of-speech tag (e.g. `VERB`),\n",
    "* `morph` - the details of the word's morphological form.\n",
    "\n",
    "Here's how we can see those annotations for a short snippet of the book's 27-th paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“\t\"\t\"\tPUNCT\tPunctSide=Ini|PunctType=Quot\n",
      "What\twhat\twhat\tPRON\t\n",
      "’s\t's\t’\tVERB\tNumber=Sing|Person=Three|Tense=Pres|VerbForm=Fin\n",
      "this\tthis\tthis\tDET\tNumber=Sing|PronType=Dem\n",
      "?\t?\t?\tPUNCT\tPunctType=Peri\n",
      "this\tthis\tthis\tDET\tNumber=Sing|PronType=Dem\n",
      "?\t?\t?\tPUNCT\tPunctType=Peri\n",
      "”\t\"\t\"\tPUNCT\tPunctSide=Fin|PunctType=Quot\n",
      "she\tshe\tshe\tPRON\tCase=Nom|Gender=Fem|Number=Sing|Person=3|PronType=Prs\n",
      "asked\tasked\task\tVERB\tTense=Past|VerbForm=Fin\n",
      ",\t,\t,\tPUNCT\tPunctType=Comm\n",
      "pointing\tpointing\tpoint\tVERB\tAspect=Prog|Tense=Pres|VerbForm=Part\n",
      "to\tto\tto\tADP\t\n",
      "the\tthe\tthe\tDET\tDefinite=Def|PronType=Art\n",
      "letter\tletter\tletter\tNOUN\tNumber=Sing\n",
      ".\t.\t.\tPUNCT\tPunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "for tok in docs[26]:\n",
    "    print(tok.orth_, tok.norm_, tok.lemma_, tok.pos_, tok.morph, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any of the labels sound cryptic, there's a very useful function called `spacy.explain()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1\n",
    "\n",
    "Count the number of occurrences of each POS-tag in a dictionary called `tag_freqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN': 56203, 'DET': 34508, 'PROPN': 14015, 'ADP': 43417, 'PUNCT': 72260, 'AUX': 20999, 'PRON': 51952, 'ADV': 23033, 'CCONJ': 16941, 'SPACE': 292, 'VERB': 56767, 'NUM': 2215, 'X': 116, 'ADJ': 21505, 'SCONJ': 6317, 'PART': 12034, 'INTJ': 2129, 'SYM': 9}\n"
     ]
    }
   ],
   "source": [
    "tag_freqs = {}\n",
    "for paragraph in docs:\n",
    "    for tok in paragraph:\n",
    "        if tok.pos_ not in tag_freqs:\n",
    "            tag_freqs[tok.pos_] = 1\n",
    "        else:\n",
    "            tag_freqs[tok.pos_] += 1\n",
    "print(tag_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x7fce0c2f2040>, {'NOUN': 56203, 'DET': 34508, 'PROPN': 14015, 'ADP': 43417, 'PUNCT': 72260, 'AUX': 20999, 'PRON': 51952, 'ADV': 23033, 'CCONJ': 16941, 'SPACE': 292, 'VERB': 56767, 'NUM': 2215, 'X': 116, 'ADJ': 21505, 'SCONJ': 6317, 'PART': 12034, 'INTJ': 2129, 'SYM': 9})\n"
     ]
    }
   ],
   "source": [
    "# Does the same but with lambda function\n",
    "tag_freqs = defaultdict(lambda: 0)\n",
    "for d in docs:\n",
    "    for tok in d:\n",
    "        tag_freqs[tok.pos_] += 1\n",
    "print(tag_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56767"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freqs['VERB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done this, we can  list the different parts of speech tags in the order of decreasing frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PUNCT', 72260),\n",
       " ('VERB', 56767),\n",
       " ('NOUN', 56203),\n",
       " ('PRON', 51952),\n",
       " ('ADP', 43417),\n",
       " ('DET', 34508),\n",
       " ('ADV', 23033),\n",
       " ('ADJ', 21505),\n",
       " ('AUX', 20999),\n",
       " ('CCONJ', 16941),\n",
       " ('PROPN', 14015),\n",
       " ('PART', 12034),\n",
       " ('SCONJ', 6317),\n",
       " ('NUM', 2215),\n",
       " ('INTJ', 2129),\n",
       " ('SPACE', 292),\n",
       " ('X', 116),\n",
       " ('SYM', 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tag_freqs.items(), reverse=True, key=itemgetter(1))\n",
    "\n",
    "# Itemgetter sorts the second item = we're sorting by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2\n",
    "\n",
    "Find the most frequently occurring nouns, verbs and adjectives (lemmas). For this, you will need to build a dictionary `lemma_by_tag`, in which each entry is a list of lemmas for this tag, ordered by frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vnořený slovník, nejprve vytvoříme první prázdný slovník, pak druhý, zanořený slovník, který obsahuje pro\n",
    "# všechny neznámé 0, pak se postupně přidává 1\n",
    "\n",
    "lemmas_by_tag = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for d in docs:\n",
    "    for tok in d:\n",
    "        lemmas_by_tag[tok.pos_][tok.lemma_] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tags in lemmas_by_tag:\n",
    "    lemmas_by_tag[tags] = sorted(lemmas_by_tag[tags].items(), reverse=True, key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 763),\n",
       " ('time', 651),\n",
       " ('hand', 603),\n",
       " ('eye', 568),\n",
       " ('face', 556),\n",
       " ('_', 470),\n",
       " ('life', 467),\n",
       " ('room', 455),\n",
       " ('day', 433),\n",
       " ('wife', 425)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_by_tag['NOUN'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 3762),\n",
       " ('say', 3437),\n",
       " ('go', 1908),\n",
       " ('’', 1415),\n",
       " ('see', 1368),\n",
       " ('have', 1332),\n",
       " ('come', 1311),\n",
       " ('know', 1259),\n",
       " ('do', 1002),\n",
       " ('look', 973)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_by_tag['VERB'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 465),\n",
       " ('little', 456),\n",
       " ('same', 448),\n",
       " ('old', 421),\n",
       " ('other', 379),\n",
       " ('own', 348),\n",
       " ('new', 316),\n",
       " ('more', 287),\n",
       " ('great', 258),\n",
       " ('first', 255)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_by_tag['ADJ'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3\n",
    "\n",
    "Find the 10 word forms most ambiguous wrt. POS tag, i.e. having the most possible tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4\n",
    "\n",
    "Test the Zipf's law (plot rank against frequency on a log-log scale) for word forms and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6 (2p.)\n",
    "\n",
    "In English, the lemma of a word is often identical to the word form. But how often? Write the function `lemma_equals_word(docs, tag)` that measures the proportion of tokens tagged with `tag`, for which the lemma is identical to the normalized word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_equals_word(docs, tag):\n",
    "    count = 0\n",
    "    count_same = 0\n",
    "\n",
    "    for d in docs:\n",
    "        for tok in d:\n",
    "            if tok.pos_ == tag:\n",
    "                count += 1\n",
    "                if tok.norm_ == tok.lemma_:\n",
    "                    count_same += 1\n",
    "    \n",
    "    print(count_same/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9610788188793304\n"
     ]
    }
   ],
   "source": [
    "lemma_equals_word(docs, 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8085155596676334\n"
     ]
    }
   ],
   "source": [
    "lemma_equals_word(docs, 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.299117444994451\n"
     ]
    }
   ],
   "source": [
    "lemma_equals_word(docs, 'VERB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 7 (3p.)\n",
    "\n",
    "Write a function `find_examples(docs, word, tags)`, which finds instances of `word` being tagged with different `tags`. Use it to show example sentences of the different taggings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_examples(docs, word, tags):\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for d in docs:\n",
    "        for tok in d:\n",
    "            if str(tok) == word:\n",
    "                results[tok.pos_].append(tok.sent)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = find_examples(docs, 'sleep', ['NOUN', 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He turned over his stout, well-cared-for person on the springy sofa, as though he would sink into a long sleep again; he vigorously embraced the pillow on the other side and buried his face in it; but all at once he jumped up, sat up on the sofa, and opened his eyes."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['NOUN'][0].sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "But after she had gone to bed, for a long while she could not sleep."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['VERB'][0].sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He turned over his stout, well-cared-for person on the springy sofa, as though he would sink into a long sleep again; he vigorously embraced the pillow on the other side and buried his face in it; but all at once he jumped up, sat up on the sofa, and opened his eyes."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['NOUN'][0].sent #dont touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "But after she had gone to bed, for a long while she could not sleep."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['VERB'][0].sent #dont touch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
